# common/vietnamese_utils.py
"""Vietnamese text preprocessing utilities."""

import re
import unicodedata
from typing import List, Dict, Optional
from underthesea import sent_tokenize, word_tokenize, ner


class VietnamesePreprocessor:
    """Vietnamese text normalization and tokenization."""

    def __init__(self):
        # Common Vietnamese abbreviations
        self.abbreviations = {
            "TPHCM": "Th√†nh ph·ªë H·ªì Ch√≠ Minh",
            "TP.HCM": "Th√†nh ph·ªë H·ªì Ch√≠ Minh",
            "HN": "H√† N·ªôi",
            "UBND": "·ª¶y ban Nh√¢n d√¢n",
            "VN": "Vi·ªát Nam",
            "TP": "Th√†nh ph·ªë",
            "PGS.TS": "Ph√≥ Gi√°o s∆∞ Ti·∫øn sƒ©",
            "TS": "Ti·∫øn sƒ©",
            "ThS": "Th·∫°c sƒ©",
            "GS": "Gi√°o s∆∞",
        }

    def normalize_text(self, text: str) -> str:
        """
        Normalize Vietnamese text:
        - Convert to NFC Unicode normalization
        - Expand abbreviations
        - Remove extra whitespace
        """
        # Normalize Unicode
        text = unicodedata.normalize('NFC', text)

        # Expand abbreviations
        for abbr, full in self.abbreviations.items():
            text = re.sub(rf'\b{re.escape(abbr)}\b', full, text, flags=re.IGNORECASE)

        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def tokenize_sentences(self, text: str) -> List[str]:
        """Sentence tokenization for Vietnamese using underthesea."""
        try:
            return sent_tokenize(text)
        except Exception as e:
            print(f"‚ö†Ô∏è Sentence tokenization error: {e}")
            # Fallback: split by common punctuation
            return [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]

    def tokenize_words(self, text: str) -> List[str]:
        """Word tokenization for Vietnamese."""
        try:
            return word_tokenize(text, format="text").split()
        except Exception as e:
            print(f"‚ö†Ô∏è Word tokenization error: {e}")
            # Fallback: simple split
            return text.split()

    def extract_entities(self, text: str) -> List[Dict]:
        """
        Extract named entities from Vietnamese text.
        Uses underthesea NER model.
        """
        try:
            entities_raw = ner(text)
            results = []

            for entity in entities_raw:
                if len(entity) >= 4 and entity[3] != 'O':  # Not "Outside" tag
                    results.append({
                        "text": entity[0],
                        "type": entity[3],  # B-PER, I-PER, B-LOC, etc.
                        "pos": entity[1] if len(entity) > 1 else "",
                    })

            return results
        except Exception as e:
            print(f"‚ö†Ô∏è Entity extraction error: {e}")
            return []

    def preprocess_claim(self, claim: str) -> Dict:
        """
        Complete preprocessing pipeline for a claim.
        Returns dict with various representations.
        """
        normalized = self.normalize_text(claim)
        sentences = self.tokenize_sentences(normalized)
        tokens = self.tokenize_words(normalized)
        entities = self.extract_entities(normalized)

        return {
            "original": claim,
            "normalized": normalized,
            "sentences": sentences,
            "tokens": tokens,
            "entities": entities,
            "token_count": len(tokens)
        }


# Global instance
preprocessor = VietnamesePreprocessor()


# Standalone testing
if __name__ == "__main__":
    # Test preprocessing
    test_claim = "TP.HCM c√≥ h∆°n 10 tri·ªáu d√¢n v√†o nƒÉm 2024"
    result = preprocessor.preprocess_claim(test_claim)
    
    print("üìù Preprocessing Test:")
    print(f"Original: {result['original']}")
    print(f"Normalized: {result['normalized']}")
    print(f"Tokens: {result['tokens'][:10]}")
    print(f"Entities: {result['entities']}")
    print(f"Token count: {result['token_count']}")
